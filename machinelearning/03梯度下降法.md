## 梯度下降法

梯度下降法是一种基于搜索的最优化方法，作用：最小化一个损失函数。

梯度上升法： 最大化一个效用函数。

![](images/ml_11.png)

移动步长 : -η

- η 称为学习率
- η的取值影响获得最优解的速度
- η的取值不合适，甚至得不到最优解
- η是梯度下降法的一个超参数


 调参 ，就是调η 。 

- 局部最优解、全局最优解。
- 并不是所有函数都有唯一的极值点 （一会下降一会上升再下降上升等)
  - 解决方案
    - 多次运行，随机初始化点
    - 梯度下降法的初始点也是个超参数。


线性回归法的损失函数具有唯一的最优解。




![](images/ml_13.png)
![](images/ml_14.png)


#### 模拟梯度下降法
[代码](gradientDescent/01-GradientDescentSimulations/01-GradientDescentSimulations.ipynb)



![](images/ml_15.png)


![](images/ml_16.png)

![](images/ml_12.png)

### 线性回归中的梯度下降法



![](images/ml_17.png)


![](images/ml_18.png)
ps:**给 theta０ 凑了个x0, 下图xb(i) * theata 是 简化，向量化方式**



这么看ｍ如果越大，损失就越大。 在梯度中是不合理的。 我们统一除以ｍ，排除这个因素


![](images/ml_19.png)


#### 梯度下降法实现

```python
# 求均方差，mes , theta 是一个数组，X_b 是一个矩阵 ，ｎ行ｉ列， n=len(X_b)
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / len(X_b)
    except:
        # 异常则给 浮点数中的最大值
        return float('inf')

# 求导 （也就是损失值、梯度), 就是上图中的三角形J(theta)
def dJ(theta, X_b, y):
    res = np.empty(len(theta))
    # 上图第０行有点特殊
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
        #  X_b[:,i] 代表， 第几列
        res[i] = (X_b.dot(theta) - y).dot(X_b[:,i])
    return res * 2 / len(X_b)


# 求梯度下降中的 最优theta 
# initial_theta 初始化theta
# n_iters 迭代次数控制
# eta 学习率
# X_b 矩阵
# y 结果值
# epsilon 接受的误差值
def gradient_descent(X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
    
    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        # 下降梯度
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if(abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
            
        cur_iter += 1

    return theta
```




