## 梯度下降法

梯度下降法是一种基于搜索的最优化方法，作用：最小化一个损失函数。

梯度上升法： 最大化一个效用函数。

![](images/ml_11.png)

移动步长 : -η

- η 称为学习率
- η的取值影响获得最优解的速度
- η的取值不合适，甚至得不到最优解
- η是梯度下降法的一个超参数


 调参 ，就是调η 。 

- 局部最优解、全局最优解。
- 并不是所有函数都有唯一的极值点 （一会下降一会上升再下降上升等)
  - 解决方案
    - 多次运行，随机初始化点
    - 梯度下降法的初始点也是个超参数。


线性回归法的损失函数具有唯一的最优解。




![](images/ml_13.png)
![](images/ml_14.png)


#### 模拟梯度下降法
[代码](gradientDescent/01-GradientDescentSimulations/01-GradientDescentSimulations.ipynb)



![](images/ml_15.png)


![](images/ml_16.png)

![](images/ml_12.png)

### 线性回归中的梯度下降法



![](images/ml_17.png)


![](images/ml_18.png)
ps:**给 theta０ 凑了个x0, 下图xb(i) * theata 是 简化，向量化方式**



这么看ｍ如果越大，损失就越大。 在梯度中是不合理的。 我们统一除以ｍ，排除这个因素


![](images/ml_19.png)


#### 梯度下降法实现

```python
# 求均方差，mes , theta 是一个数组，X_b 是一个矩阵 ，ｎ行ｉ列， n=len(X_b)
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta))**2) / len(X_b)
    except:
        # 异常则给 浮点数中的最大值
        return float('inf')

# 求导 （也就是损失值、梯度), 就是上图中的三角形J(theta)
def dJ(theta, X_b, y):
    res = np.empty(len(theta))
    # 上图第０行有点特殊
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
        #  X_b[:,i] 代表， 第几列
        res[i] = (X_b.dot(theta) - y).dot(X_b[:,i])
    return res * 2 / len(X_b)


# 求梯度下降中的 最优theta 
# initial_theta 初始化theta
# n_iters 迭代次数控制
# eta 学习率
# X_b 矩阵
# y 结果值
# epsilon 接受的误差值
def gradient_descent(X_b, y, initial_theta, eta, n_iters = 1e4, epsilon=1e-8):
    
    theta = initial_theta
    cur_iter = 0

    while cur_iter < n_iters:
        # 下降梯度
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if(abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
            
        cur_iter += 1

    return theta
```




#### 上面公式转成向量化运算方式
![](images/ml_20.png)

公式变成了 ２个矩阵相乘 

(1行，m列) 的行向量  乘  （m行，n+1列矩阵)

注意要把列向量转乘行向量，默认为列向量

**新的梯度公式**
```python
def dJ(theta, X_b, y):
      # 公式
      return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)
```



**利用梯度下降中，eta值要小，过大会导致得不到结果不收敛，太小导致，运行时长太长了** 

**数据还要做归一化**


使用梯度下降法寻找最小损失值的时候，数据要做归一化处理。 

在数据集中，梯度下降法比正规方程 解线性回归要快很多。 



### 随机梯度下降法

上面的梯度是对所有样本ｍ 进行 梯度计算。 如果ｍ 很大，那么 这个算法可能也是很费时的。

随机梯度下降算法(SG), 这里是取一个样本，公式变乘下图所示

![](images/ml_21.png)


模拟退火的思想 

学习率 ＝ t0/  （i_iters (迭代次数) +t1)  

经验值t0=5, t1=50


```python
# 求随机梯队，  公式
# X_b_i  X_b中的 第ｉ个样本 只有一行记录
def dJ_sgd(theta, X_b_i, y_i):
    return 2 * X_b_i.T.dot(X_b_i.dot(theta) - y_i)

def sgd(X_b, y, initial_theta, n_iters):
    # 经验值
    t0, t1 = 5, 50
    # 学习率 随着迭代次数（时间） ，慢慢变小。 
    def learning_rate(t):
        return t0 / (t + t1)

    theta = initial_theta
    for cur_iter in range(n_iters):
        rand_i = np.random.randint(len(X_b))
        gradient = dJ_sgd(theta, X_b[rand_i], y[rand_i])
        theta = theta - learning_rate(cur_iter) * gradient

    return theta
```


### 梯度下降法相对好的做法

**迭代所有样本 ｎ次循环， 默认是５，然后循环的时候 每个样本都随机打乱，并且每一轮数据都要取到** 




###  梯度下降法调试手段

点的梯度 ＝ ２点间， 无限趋近与０ 的斜切率。  


![](images/ml_25.png)

![](images/ml_26.png)

